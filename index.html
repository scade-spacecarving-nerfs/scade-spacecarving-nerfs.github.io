<!DOCTYPE html>
<html lang="en">
  <head>

<!--   <link rel=”stylesheet” href=”https://stackpath.bootstrapcdn.com/bootstrap/5.0.0-alpha1/css/bootstrap.min.css”rel=”nofollow” integrity=”sha384-r4NyP46KrjDleawBgD5tp8Y7UzmLA05oM1iAEQ17CSuDqnUK2+k9luXQOfXJCJ4I” crossorigin=”anonymous”>
 -->

    <br>
    <!-- Basic Page Needs
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <meta charset="utf-8">
    <title>SCADE</title>
    <meta name="description" content="">
    <meta name="author" content="">

    <!-- Mobile Specific Metas
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- FONT
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link href="https://fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

    <!-- CSS
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link rel="stylesheet" href="css/normalize.css">
    <link rel="stylesheet" href="css/skeleton.css">
    <link rel="stylesheet" href="css/footable.standalone.min.css">
    <!-- <link rel="stylesheet" href="css/bootstrap.min.css"> -->

    <!-- Favicon
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <link rel="icon" type="image/png" href="images/favicon.png">

    <!-- Google icon -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">

    <!-- Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                               m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                              })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-86869673-1', 'auto');
      ga('send', 'pageview');
    </script>


    <!-- <script src=”https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js” integrity=”sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo” crossorigin=”anonymous”></script>
    <script src=”https://stackpath.bootstrapcdn.com/bootstrap/5.0.0-alpha1/js/bootstrap.min.js” integrity=”sha384-oesi62hOLfzrys4LxRF63OJCXdXDipiYWBnvTl9Y9/TRlw5xlKIEHpNyvvDShgf/” crossorigin=”anonymous”></script> -->


    <!-- Hover effect: https://codepen.io/nxworld/pen/ZYNOBZ -->
    <style>
      img {
          display: block;
      }

      .column-50 {
          float: left;
          width: 50%;
      }
      .row-50:after {
          content: "";
          display: table;
          clear: both;
      }

      .floating-teaser {
          float: left;
          width: 30%;
          text-align: center;
          padding: 15px;
      }
      .venue strong {
          color: #99324b;
      }

      .benchmark {
          width: 100%;
          max-width: 960px;
          overflow: scroll;
          overflow-y: hidden;
      }

    </style>
  </head>
  <body>

    <!-- Primary Page Layout
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="container">
      <h4 style="text-align:center">SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates</h4>
      <p align="center", style="margin-bottom:12px;">
        <a class="simple" href="https://mikacuy.github.io/">Mikaela Angelina Uy</a><sup>1</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="http://ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a><sup>1, 2</sup>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
        <a class="simple" href="https://www.sfu.ca/~keli/">Ke Li</a><sup>2,3</sup>
      </p>

      <p align="center" style="margin-bottom:20px;">
        <sup>1</sup>Stanford University
        <span style="display:inline-block; width: 32px"></span>
        <sup>2</sup>Google
        <span style="display:inline-block; width: 32px"></span>
        <sup>3</sup>Simon Fraser University        
        <br>
      </p>

      <div class="venue">
        <p align="center">Conference on Computer Vision and Pattern Recognition (CVPR), 2023</p>
      </div>

      <figure>
        <img src="images/scade_teaser_corrected_v2.png" style="width:100%"></img>
        <!-- <br> -->
      </figure>
      <div class="caption">
        <b>SCADE Overview</b>. We present SCADE, a novel technique for NeRF reconstruction under <b><em>sparse, unconstrained views</em></b> for in-the-wild indoor scenes. We leverage on generalizable monocular depth priors and address to represent the inherent ambiguities of monocular depth by exploiting our <b><em>ambiguity-aware depth estimates</em></b> (leftmost). Our approach accounts for <b><em>multimodality</em></b> of both distributions using our novel <b><em>space carving loss</em></b> that seeks to <b><em>disambiguate</em></b> and find the common mode to <b><em>fuse</em></b> the information between different views (middle). As shown (rightmost), SCADE enables better photometric reconstruction especially in highly ambiguous scenes such as non-opaque surfaces.
      </div>

      <br><br>

      <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
        <h5>Abstract</h5>
        <p align="justify">
          Neural radiance fields (NeRFs) have enabled high fidelity 3D reconstruction from multiple 2D input views. However, a well-known drawback of NeRFs is the less-than-ideal performance under a small number of views, due to insufficient constraints enforced by volumetric rendering. To address this issue, we introduce <b>SCADE</b>, a novel technique that improves NeRF reconstruction quality on sparse, unconstrained input views for in-the-wild indoor scenes. To constrain NeRF reconstruction, we leverage geometric priors in the form of per-view depth estimates produced with state-of-the-art monocular depth estimation models, which can generalize across scenes. A key challenge is that monocular depth estimation is an ill-posed problem, with inherent <b>ambiguities</b>. To handle this issue, we propose a new method that learns to predict, for each view, a continuous, multimodal <b>distribution</b> of depth estimates using <em>conditional Implicit Maximum Likelihood Estimation</em> (cIMLE). In order to disambiguate exploiting multiple views, we introduce an original <b>space carving loss</b> that guides the NeRF representation to <b>fuse</b> multiple hypothesized depth maps from each view and distill from them a common geometry that is consistent with all views. Experiments show that our approach enables higher fidelity novel view synthesis from sparse views.
          <br>
          <br>
        </p>
      </div>

      <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
        <h5>Video</h5>
      <center>
      <!-- <p>Coming Soon!</p> -->
      <iframe width="560" height="315" src="https://www.youtube.com/embed/5XwWZn-kjBU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
      </center> 
      </div>


    <div class="section">
        <h5>Materials</h5>
        <div class="container" style="width:95%">
          <!-- Icon row -->
          <div class="row">
            <div class="two columns">
              <a href="https://arxiv.org/pdf/2303.13582.pdf"><img style="border: 1px solid #ddd; border-radius: 4px; padding: 2px; width: 108px;" src="images/screenshot_main.png"></a>
            </div>
            <div class="two columns">
              <a href="assets/scade_final_slides.pdf"><img style="border: 1px solid #ddd; border-radius: 4px; padding: 2px; width: 108px;" src="images/screenshot_slides.png"></a>
            </div>            
            <div class="two columns">
              <a href="assets/scade_cvpr23_poster_final.pdf"><img style="border: 1px solid #ddd; border-radius: 4px; padding: 2px; width: 280px;" src="images/screenshot_poster.png"></a>
            </div>
          </div>
          <!-- Link row -->
          <div class="row">
            <div class="two columns">
              <a href="https://arxiv.org/pdf/2303.13582.pdf">Paper</a>
            </div>
            <div class="two columns">
              <a href="assets/scade_final_slides.pdf">Slides</a>
            </div>
            <div class="two columns">
              <a href="assets/scade_cvpr23_poster_final.pdf">Poster</a>
            </div>
          </div>

          <!-- <div class="row"> -->
              <!-- <br> -->
              <!-- <a href="https://github.com/mikacuy/point2cyl">Code</a> -->
              <!-- <p>Paper to be released soon!</p> -->
          <!-- </div> -->

          <div class="row">
              <br>
              <!-- <a href="https://github.com/mikacuy/point2cyl">Code</a> -->
              <p>Code to be released soon!</p>
          </div>

        </div>
      </div>

  <br>



<div class="col-md-8 col-md-offset-2">
    <h5>Demo</h5>
<div id="Demo" class="carousel">
  <div class="carousel-inner">
    <div class="carousel-item active">
      <video class="carousel-video" autoplay muted loop>
          <source src="video/scannet_demo.mp4" type="video/mp4">
        </video>
    </div>
    <div class="carousel-item">
      <video class="carousel-video"  autoplay muted loop playsinline>
          <source src="video/glass_scene_demo.mp4" type="video/mp4">
      </video>
    </div>

    <div class="carousel-item">
      <video class="carousel-video" autoplay muted loop playsinline>
          <source src="video/church_demo.mp4" type="video/mp4">
      </video>
    </div>

  </div>

  <a id="carouselPrev" class="carousel-control-prev" href="#Demo" role="button">
      <span class="carousel-control-prev-icon" aria-hidden="true"></span>
      <span class="sr-only">Previous</span>
    </a>
  <a id="carouselNext" class="carousel-control-next" href="#Demo" role="button">
      <span class="carousel-control-next-icon" aria-hidden="true"></span>
      <span class="sr-only">Next</span>
    </a>
</div>

<script>
  const carousel = document.getElementById("Demo");
  const items = carousel.querySelectorAll(".carousel-item");
  let currentItem = 0;
  let isActive = true;

  function setCurrentItem(index) {
    currentItem = (index + items.length) % items.length;
  }

  function hideItem(direction) {
    isActive = false;
    items[currentItem].classList.add(direction);
    items[currentItem].addEventListener("animationend", function() {
      this.classList.remove("active", direction);
    });
  }

  function showItem(direction) {
    items[currentItem].classList.add("next", direction);
    items[currentItem].addEventListener("animationend", function() {
      this.classList.remove("next", direction);
      this.classList.add("active");
      isActive = true;
    });
  }

  document.getElementById("carouselPrev").addEventListener("click", function(e) {
    e.preventDefault();
    if (isActive) {
      hideItem("to-right");
      setCurrentItem(currentItem - 1);
      showItem("from-left");
    }
  });

  document.getElementById("carouselNext").addEventListener("click", function(e) {
    e.preventDefault();
    if (isActive) {
      hideItem("to-left");
      setCurrentItem(currentItem + 1);
      showItem("from-right");
    }
  });  
</script>

<p align="justify">
  Toggle arrows to browse through demos from the different datasets.
</p>

<!-- <button onclick="myFunction1()">Scannet</button>

<div id="myDIV1">
    <video id="v0" width="90%" autoplay="" loop="" muted="" controls="">
      <source src="video/scannet_demo.mp4" type="video/mp4">
    </video>
</div>

<script>
function myFunction1() {
  var x = document.getElementById("myDIV1");
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script>

<br>

<button onclick="myFunction2()">In-the-Wild</button>

<div id="myDIV2">
    <video id="v0" width="90%" autoplay="" loop="" muted="" controls="">
      <source src="video/glass_scene_demo.mp4" type="video/mp4">
    </video>
</div>
<script>
function myFunction2() {
  var x = document.getElementById("myDIV2");
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script>


<br>

<button onclick="myFunction3()">Tanks and Temples</button>

<div id="myDIV3">
    <video id="v0" width="90%" autoplay="" loop="" muted="" controls="">
      <source src="video/church_demo.mp4" type="video/mp4">
    </video>    
</div>

<br>

<script>
function myFunction3() {
  var x = document.getElementById("myDIV3");
  if (x.style.display === "none") {
    x.style.display = "block";
  } else {
    x.style.display = "none";
  }
}
</script> -->


    <!-- <h6>Scannet</h6><br> -->
<!--     <video id="v0" width="90%" autoplay="" loop="" muted="" controls="">
      <source src="video/scannet_demo.mp4" type="video/mp4">
    </video> -->
    <!-- <h6>In the Wild dataset</h6><br> -->
<!--     <video id="v0" width="90%" autoplay="" loop="" muted="" controls="">
      <source src="video/glass_scene_demo.mp4" type="video/mp4">
    </video> -->
    <!-- <h6>Tanks and Temples</h6><br> -->
<!--     <video id="v0" width="90%" autoplay="" loop="" muted="" controls="">
      <source src="video/church_demo.mp4" type="video/mp4">
    </video>     -->


</div>


<div class="col-md-8 col-md-offset-2">
    <h5>Qualitative Results</h5>
<div id="Qualitative_Results" class="carousel2">
  <div class="carousel-inner">
    <div class="carousel-item active">
      <img class="carousel-img" src="images/results_scannet.png">
    </div>
    <div class="carousel-item">
      <img class="carousel-img" src="images/results_wild.png">
    </div>

    <div class="carousel-item">
      <img class="carousel-img" src="images/results_tnt.png">
    </div>

  </div>

  <a id="carouselPrev2" class="carousel-control-prev" href="#Qualitative_Results" role="button">
      <span class="carousel-control-prev-icon" aria-hidden="true"></span>
      <span class="sr-only">Previous</span>
    </a>
  <a id="carouselNext2" class="carousel-control-next" href="#Qualitative_Results" role="button">
      <span class="carousel-control-next-icon" aria-hidden="true"></span>
      <span class="sr-only">Next</span>
    </a>
</div>

<script>
  const carousel2 = document.getElementById("Qualitative_Results");
  const items2 = carousel2.querySelectorAll(".carousel-item");
  let currentItem2 = 0;
  let isActive2 = true;

  function setCurrentItem2(index) {
    currentItem2 = (index + items2.length) % items2.length;
  }

  function hideItem2(direction) {
    isActive2 = false;
    items2[currentItem2].classList.add(direction);
    items2[currentItem2].addEventListener("animationend", function() {
      this.classList.remove("active", direction);
    });
  }

  function showItem2(direction) {
    items2[currentItem2].classList.add("next", direction);
    items2[currentItem2].addEventListener("animationend", function() {
      this.classList.remove("next", direction);
      this.classList.add("active");
      isActive2 = true;
    });
  }

  document.getElementById("carouselPrev2").addEventListener("click", function(e) {
    e.preventDefault();
    if (isActive2) {
      hideItem2("to-right");
      setCurrentItem2(currentItem2 - 1);
      showItem2("from-left");
    }
  });

  document.getElementById("carouselNext2").addEventListener("click", function(e) {
    e.preventDefault();
    if (isActive2) {
      hideItem2("to-left");
      setCurrentItem2(currentItem2 + 1);
      showItem2("from-right");
    }
  });  
</script>

<br><br>
<p align="justify">
  Toggle arrows to browse through results from the different datasets.
</p>

</div>



<br>
<br>

  <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
    <h5>Overview</h5>
    <center>
      <img src="images/depth_ambiguities.png" style="width:70%"></img>
        <div class="caption" >
          <p align="justify">
            The main idea is to leverage depth as a prior in order to constrain nerf optimization under the sparse view regime. However, when estimating depth from a single image, there can be multiple, equally valid depth estimates, these arise from inherent <b>ambiguities</b> such as albedo vs shading, scale, or non-opaque surfaces.
          </p>
        </div>
        <br>
    </center>
  </div>

  <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
    <h5>Our Ambiguity-Aware Prior</h5>
    <center>
      <img src="images/architecture.png" style="width:70%"></img>
        <div class="caption" >
          <p align="justify">
            We retain these ambiguities at the stage of monocular depth estimation by representing depth as a <b>distribution</b>, which can be multimodal, represented with samples using <b>conditional Implicit Maximum Likelihood Estimation</b> (cIMLE) to capture variable depth modes. The network architecture is a simple combination of a state-of-the-art monocular depth estimation network, LeReS, with cIMLE that is implemented by adding AdaIn layers to the encoder. This allows us to output a distribution of possible depths from a single image, represented as a set of samples.
          </p>
        </div>
        <br>
    </center>
  </div>

  <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
    <h5>Ambiguity-Aware Depth Estimates</h5>
    <center>
      <img src="images/ambiguity_aware_prior_estimates.png" style="width:70%"></img>
        <div class="caption" >
          <p align="justify">
            We find that our ambiguity aware prior is able to capture the different ambiguities, such as albedo vs shading (top), different scales of concavity (middle) and multimodality in the outputs of non-opaque surfaces (bottom).
          </p>
        </div>
        <br>
    </center>
  </div>

  <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
    <h5>Why does it work?</h5>
    <center>
      <img src="images/taskonomy_reflective_glass.png" style="width:70%"></img>
        <div class="caption" >
          <p align="justify">
            We achieve variable depth modes by exploiting inconsistently labeled training data. In the Taskonomy dataset that was used to train our prior, different training images with non-opaque surfaces label depth differently: shooting through the glass (left), on the glass (middle), or a mixture of both (right). Despite multiple possible depth labels, each image only has one ground truth label. Training with cIMLE allows our prior to model these multiple possible (ambiguous) outputs through sampling, even when given only one label per image. We also observe that our trained prior is also able to capture variable modes on reflective surfaces that typical monocular depth estimation networks struggle on.
          </p>
        </div>
        <br>
    </center>
  </div>

  <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
    <h5>Space Carving Intuition</h5>
    <center>
      <img src="images/space_carving_illustration.png" style="width:70%"></img>
        <div class="caption" >
          <p align="justify">
            When given a sparse set of views, we are able to resolve the ambiguities by fusing information from the multiple views. To do so, we draw inspiration from the classical space carving. For every view, illustrated by the green and yellow cameras, we have multiple depth hypothesis from our ambiguity-aware prior. For simplicity, let’s assume there are two hypothesis per camera (labelled 1 and 2). Space carving eliminates inconsistent hypotheses: E.g. green hypothesis 2 is not consistent with both yellow hypotheses because if it were consistent, everything in front of the green camera cone should be empty.

          </p>
        </div>
        <br>
    </center>
  </div>

  <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
    <h5>SCADE</h5>
    <center>
      <img src="images/scade_method.png" style="width:90%"></img>
        <div class="caption" >
          <p align="justify">
            Thus in SCADE, we <b>distill</b> the <b>consistent</b> hypotheses for each view into a global 3D geometry represented with a NeRF. To achieve this, we introduce our novel <b>space carving loss</b> function on the two distributions: our ambiguity-aware prior and the ray termination distance of NeRF, that is differentiable and sample-based. Our space carving loss drives the learned depth distribution from the NeRF to be consistent with <b>some</b> depth hypothesis in <b>every</b> view. Our resulting loss function is <b>mode seeking</b>. Moreover, as opposed to existing works which supervise 2D moments, we <b>supervise at individual samples</b> along each ray in 3D.
          </p>
        </div>
        <br>
    </center>
  </div>


  <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
    <h5>Additional Results</h5>
<!--     <center>
      <img src="images/results_wild.png" style="width:90%"></img>
        <div class="caption" >
          <p align="justify">
            Qualitative examples from In-the-Wild dataset. As shown, we are able to recover the room behind the glass better the baselines, whose results are more blurry (1st, 3rd, 4th column). Also, SCADE is able to recover crisp shapes of objects such as the thin table leg and the white chair (2nd, last column), and we are also able to clear up dust near the microwave and printer compared to the baselines (5th, last column).
          </p>
        </div>
        <br>
    </center>
    <center>
      <img src="images/results_scannet.png" style="width:90%"></img>
        <div class="caption" >
          <p align="justify">
            Qualitative examples from Scannet. As shown, SCADE is able to avoid producing the clouds of dust that are present in the results of baselines (1st, 3rd, last columns). Moreover, SCADE is also able to snap and recover objects in the scene such as the details on the blinds (2nd column), the back of the chair (4th column) and the legs of the piano stool (5th column).
          </p>
        </div>
        <br>
    </center>
    <center>
      <img src="images/results_tnt.png" style="width:90%"></img>
        <div class="caption" >
          <p align="justify">
            More qualitative examples from the Tanks and Temples dataset.
          </p>
        </div>
        <br>
    </center> -->
    <center>
      <img src="images/results_depth.png" style="width:90%"></img>
        <div class="caption" >
          <p align="justify">
            Samples from our Ambiguity-Aware Depth Estimates on train images of the different scenes used in our experiments. Ambiguity is shown in [Left; right]: (a) How far the back wall is relative to the chair as well as the width of the cabinet and how far it is relative to the desk; whether the door is at a different compared to the wall and the relative depth of the the second chair w.r.t. to the nearer chair and the wall. (b) Depth of the bookshelf; albedo v.s. shading of the door w.r.t to the door frame. (c) Whether the painted texture is convex or is flat (i.e. just painted) on the wall; whether there is a far back door or is just a texture on the wall. (d) Non-opaque surface ambiguity due to the glass cabinet; glass door behind the sofa is also non-opaque. 
          </p>
        </div>
        <br>
    </center> 
    <center>
      <img src="images/results_fusion.png" style="width:70%"></img>
        <div class="caption" >
          <p align="justify">
            We also rendered depthmaps and fusion results using TSDF Fusion on a Scannet scene. Notice that SCADE is able to recover better geometry compared to DDP -- see corner of the calendar, cabinets and office chair in the right image..
          </p>
        </div>
        <br>
    </center>       
  </div>

<!--   <div id="teaser" class="container" style="width:100%; margin:0; padding:0">
    <h5>Qualitative Results</h5>
    <center>
      <img src="images/SketchExtrudeFigure.png" style="width:90%"></img>
        <div class="caption" >
          <p align="justify">
            Visualization of retrieval followed by deformation on ShapeNet. Our network is able to retrieve models
            that better fit after deformation despite having large geometric distances initially. Notice the big back part of the retrieved chair and the thick seat of the retrieved sofa, attributes that are not identical to the query. Yet, these parts properly fit the target after deformation. Our network is also able to retrieve a sofa with legs and a car with a trunk that are present in the desired targets. Moreover, our deformation-aware retrieval & deformation approach also allows us to preserve fine-details of the source model post-deformation as shown in the zoomed in regions.
          </p>
        </div>
        <br>

      <img src="images/scan2cad_fixed_compressed.png" style="width:90%"></img>
        <div class="caption" >
          <p align="justify">
            (Top row and bottom left) Qualitative results of Scan-to-CAD. (Bottom right) Quantitative Results: We compare different retrieval methods on the Scan2CAD dataset. The left chart shows the mean fitting errors and the right chart shows the number of best candidates retrieved by different methods. Ours-Reg achieves the minimum overall fitting errors and the maximum number of best retrievals among all categories compared with other methods. 
          </p>
        </div>
        <br>

      <img src="images/image2cad3_compressed.png" style="width:90%"></img>
        <div class="caption" >
          <p align="justify">
            Qualitative results to show the feasibility of our approach for the Image-toCAD application. We show one of three input viewpoints used by Pixel2Mesh++ to produce their coarse mesh. We use this to retrieve a CAD model, which is then
            deformed to fit the coarse mesh. Rigidity constraints ensure the quality of our output
            as shown. 
          </p>
        </div>
        <br>

    </center>
  </div> -->

		<!-- -->
	<div class="section">
          <h5>Citation</h5> 
  <pre style="margin:0"><code>@inproceedings{uy-scade-cvpr23,
      title = {SCADE: NeRFs from Space Carving with Ambiguity-Aware Depth Estimates},
      author = {Mikaela Angelina Uy and Ricardo Martin-Brualla and Leonidas Guibas and Ke Li},
      booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
      year = {2023}
  }</code></pre>    			
		</div>
		
        <!-- -->
        <br> 
        
<!--   <div class="section">
      <h5>Acknowledgements</h5>            
      <p>
      This work is supported by ARL grant W911NF-21-2-0104, a Vannevar Bush Faculty Fellowship, and gifts from the Autodesk and Adobe corporations. M. Sung also acknowledges the support by NRF grant (2021R1F1A1045604) and NST grant (CRC 21011) funded by the Korea government(MSIT) and grants from the Adobe and KT corporations.
			</p>
  </div> -->

  </div>

    <script type="text/javascript" src="../js/jquery.min.js"></script>
    <script type="text/javascript" src="../js/footable.min.js"></script>
    <!-- <script type="text/javascript" src="../js/bootstrap.min.js"></script> -->

    <script type="text/javascript">
      jQuery(function($){
          $('.table').footable();
      });
    </script>

    <!-- End Document
         –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  </body>
</html>
